# Practical Machine Learning Course Project  

## SYNOPSIS  

## Data Sources

The data for this project comes from this original source: http://groupware.les.inf.puc-rio.br/har. 

The training and test data for this project are available here:  
[Training Data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)  
[Test Data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)  


## Modeling

##### The following steps where taken:  
1. Remove columns with little/no data to tidy up.  
2. Create Training and test data from traing data for cross validation checking.  
3. Trial 3 methods Random Forest, GBM and LDA and decided to choose Random Forest method as the Accuracy is greater than the other 2 methods.  


## Data Processing  
```{r echo=FALSE, results='hide'}
library(ggplot2); library(caret); library(randomForest);
library(e1071); library(gbm); library(doParallel); 
library(survival); library(splines); library(plyr)
setwd("E:/Mg Hla/Coursera")
```  

##### Load the data. Then, remove the first 6 columns as these can be ignored.  

``` {r echo=TRUE, results='hide'}
trainData <- read.csv("data/pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
testData <- read.csv("data/pml-testing.csv", na.strings=c("NA","#DIV/0!", ""))

training <- trainData[, 6:dim(trainData)[2]]
testing <- testData[, 6:dim(testData)[2]]
```  

##### Remove columns with more than 95% of NA or "" values from both Training and Testing Data sets.  

``` {r echo=TRUE, results='hide'}
treshold <- dim(training)[1] * 0.95

goodColumns_training <- !apply(training, 2, function(x) sum(is.na(x)) > treshold  || sum(x=="") > treshold)
training <- training[, goodColumns_training]
training_badColumns <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[, training_badColumns$nzv==FALSE]
training$classe = factor(training$classe)

training_colname <- colnames(training)
testing_badColumns <- nearZeroVar(testing, saveMetrics = TRUE)
testing <- testing[, testing_badColumns$nzv==FALSE]
testing$classe <- NA
testing <- testing[training_colname]
``` 

##### Partition 70% of Training Data set to train the model and the remaining 30% for crossvalidation
```{r echo=TRUE, results='hide', }
inTrain <- createDataPartition(training$classe, p = 0.7)[[1]]
crossv <- training[-inTrain,]
training <- training[ inTrain,]
```  

##### Train the model using Random Forest Method. 
```{r }
model <- train(classe ~ ., data=training, method="rf")
```  

##### Predict the model using crossvalidating data set.  
```{r}
predTrain <- predict(model, crossv)
```  

##### Confusion matrix of crossvalidation data set.  
```{r}
confusionMatrix(predTrain, crossv$classe)
``` 

##### Calculate the accuracy.  
```{r}
accuracy <- sum(predTrain == crossv$classe) / length(predTrain)
accuracy;
```  

##### Finally, apply the model on Testing Data set.  

```{r}
predTest <- predict(model, testing)
```  

###### Generate files to submit. 

```{r}
pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}

pml_write_files(predTest)

```  